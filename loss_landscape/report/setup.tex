%!TEX root=main.tex
\section{Setup}
\label{sec:setup}

\textbf{Task.} 
To be consistent with previous work of \cite{li2018visualizing}, all the experiments of this project are conducted with CIFAR-10 dataset.   

\textbf{Neural network backbone.} 
Due to the limit of computation resource, we use ResNet-20, the smallest network architecture of ResNets on CIFAR-10,  as the backbone to conduct experiment results. 
For implementation details and pretrained models, we refer the readers to the github repository of \cite{Idelbayev18a} which provides a valid pytorch implementation of ResNet-s for CIFAR-10 as described in \cite{he2016deep}.

\textbf{Number of training epochs.} 
In the original implementation, the network is trained for $200$ epochs in total, where the first $100$ epochs starts with large learning rate and the later epochs use much smaller learning rates. 
Therefore, the network weights at the end of first $100$ epochs are very close to the final ones after $200$ epochs. 
With this observation, we only train for $120$ epochs in our experiment and visualize the loss landscape around the parameters achieved. 
Specifically, the learning rate was set at $0.1$ for the first $100$ epochs, and decreased to $0.01$ for the rest $20$ epochs. 

\textbf{Data augmentation.} 
We noticed that the original implementation already applies the \textit{RandomHorizontalFlip} and \textit{RandomCrop} for data augmentation. 
To evaluate the effect of various data augmentation methods, we select two other transforms \textit{GaussianBlur} and \textit{ColorJitter} with fixed hyper parameters. 
The details are deferred to \pref{sec:data_augment}. 

\textbf{Visualization.}
We reuse the code for plotting with following changes. For projection directions, similar to \cite{li2018visualizing}, we select PCA of model checkpoint differences as the directions to project the high-dimensional loss surface into a 2-D loss landscape. 
We select $50$ values per axis, where the range of each axis is adjusted to contain all the projected points of historical weights from the stored checkpoints. 
Therefore, we are able to plot the sequence of (projected) weights together with the 2-D loss landscape. 


